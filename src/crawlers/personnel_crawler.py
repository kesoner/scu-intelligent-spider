"""
äººäº‹è³‡æ–™çˆ¬èŸ²æ¨¡çµ„

å°ˆé–€ç”¨æ–¼çˆ¬å–æ±å³å¤§å­¸äººäº‹å®¤çš„å„é¡å…¬å‘Šæ–‡ä»¶
"""

from typing import Dict, Any, List, Set
from collections import defaultdict
from loguru import logger

from .base_crawler import BaseCrawler
from ..utils import FileHandler


class PersonnelCrawler(BaseCrawler):
    """äººäº‹è³‡æ–™çˆ¬èŸ²"""
    
    def __init__(self, config_manager):
        """
        åˆå§‹åŒ–äººäº‹è³‡æ–™çˆ¬èŸ²
        
        Args:
            config_manager: é…ç½®ç®¡ç†å™¨
        """
        super().__init__(config_manager)
        
        self.base_url = self.config.get_base_url()
        self.menu_url = self.base_url + self.config.get("websites.personnel.menu_url")
        self.categories = self.config.get_personnel_categories()
        self.doc_extensions = self.config.get_document_extensions()
        
        # è³‡æ–™å„²å­˜
        self.pdf_links_set: Set[str] = set()
        self.attachments_data: Dict[str, List[str]] = defaultdict(list)
    
    def crawl(self) -> Dict[str, Any]:
        """
        åŸ·è¡Œäººäº‹è³‡æ–™çˆ¬èŸ²ä»»å‹™
        
        Returns:
            çˆ¬å–çµæœçµ±è¨ˆ
        """
        logger.info("é–‹å§‹çˆ¬å–äººäº‹è³‡æ–™")
        
        try:
            # å–å¾—ä¸»é¸å–®é é¢
            menu_soup = self.get_page_content(self.menu_url)
            if not menu_soup:
                logger.error("ç„¡æ³•å–å¾—ä¸»é¸å–®é é¢")
                return {"success": False, "error": "ç„¡æ³•å–å¾—ä¸»é¸å–®é é¢"}
            
            # è§£æé¡åˆ¥é€£çµ
            category_links = self._parse_category_links(menu_soup)
            if not category_links:
                logger.error("æœªæ‰¾åˆ°ä»»ä½•é¡åˆ¥é€£çµ")
                return {"success": False, "error": "æœªæ‰¾åˆ°ä»»ä½•é¡åˆ¥é€£çµ"}
            
            # çˆ¬å–æ¯å€‹é¡åˆ¥
            total_documents = 0
            for title, href in category_links:
                logger.info(f"ğŸ”¹ è™•ç†é¡åˆ¥: {title}")
                
                category_count = self._crawl_category(title, href)
                total_documents += category_count
                
                logger.info(f"é¡åˆ¥ '{title}' å®Œæˆï¼Œæ‰¾åˆ° {category_count} å€‹æ–‡ä»¶")
            
            # å„²å­˜çµæœ
            self._save_results()
            
            result = {
                "success": True,
                "total_categories": len(category_links),
                "total_documents": total_documents,
                "unique_pdf_links": len(self.pdf_links_set)
            }
            
            logger.info(f"äººäº‹è³‡æ–™çˆ¬å–å®Œæˆ: {result}")
            return result
            
        except Exception as e:
            logger.error(f"äººäº‹è³‡æ–™çˆ¬å–å¤±æ•—: {e}")
            return {"success": False, "error": str(e)}
    
    def _parse_category_links(self, soup) -> List[tuple]:
        """
        è§£æä¸»é¸å–®ä¸­çš„é¡åˆ¥é€£çµ
        
        Args:
            soup: ä¸»é¸å–®é é¢çš„ BeautifulSoup ç‰©ä»¶
            
        Returns:
            (æ¨™é¡Œ, é€£çµ) çš„åˆ—è¡¨
        """
        try:
            nodes = soup.select(".group a")
            
            titles = [node.text.strip() for node in nodes]
            hrefs = [node.get("href") for node in nodes]
            
            category_links = list(zip(titles, hrefs))
            logger.info(f"æ‰¾åˆ° {len(category_links)} å€‹é¡åˆ¥")
            
            return category_links
            
        except Exception as e:
            logger.error(f"è§£æé¡åˆ¥é€£çµå¤±æ•—: {e}")
            return []
    
    def _crawl_category(self, title: str, href: str) -> int:
        """
        çˆ¬å–å–®ä¸€é¡åˆ¥çš„æ‰€æœ‰é é¢
        
        Args:
            title: é¡åˆ¥æ¨™é¡Œ
            href: é¡åˆ¥é€£çµ
            
        Returns:
            æ‰¾åˆ°çš„æ–‡ä»¶æ•¸é‡
        """
        try:
            # å»ºç«‹å®Œæ•´ URL
            category_url = href if href.startswith("http") else self.base_url + href
            
            # å–å¾—ç¬¬ä¸€é ä»¥ç¢ºå®šç¸½é æ•¸
            first_page_soup = self.get_page_content(category_url)
            if not first_page_soup:
                logger.warning(f"ç„¡æ³•å–å¾—é¡åˆ¥é é¢: {category_url}")
                return 0

            max_page = self.web_parser.get_max_page(first_page_soup)
            logger.info(f"    é¡åˆ¥ '{title}' ç¸½å…± {max_page} é ")

            document_count = 0

            # çˆ¬å–æ¯ä¸€é 
            for page in range(1, max_page + 1):
                page_url = f"{category_url}?page={page}"
                logger.info(f"    â–¶ æŠ“å–ç¬¬ {page} é : {page_url}")

                page_count = self._crawl_page(page_url, title)
                document_count += page_count

            return document_count

        except Exception as e:
            logger.error(f"çˆ¬å–é¡åˆ¥ '{title}' å¤±æ•—: {e}")
            return 0

    def _crawl_page(self, page_url: str, category_title: str) -> int:
        """
        çˆ¬å–å–®ä¸€é é¢çš„æ–‡ä»¶é€£çµ

        Args:
            page_url: é é¢ URL
            category_title: é¡åˆ¥æ¨™é¡Œ

        Returns:
            æ‰¾åˆ°çš„æ–‡ä»¶æ•¸é‡
        """
        try:
            page_soup = self.get_page_content(page_url)
            if not page_soup:
                logger.warning(f"ç„¡æ³•å–å¾—é é¢: {page_url}")
                return 0

            # æ‰¾åˆ°æ‰€æœ‰é€£çµ
            inner_links = page_soup.select("tbody a") or page_soup.select("#rndbox_body a")
            filtered_links = self.web_parser.filter_navigation_links(inner_links)

            document_count = 0

            for node in filtered_links:
                document_title = node.text.strip()

                # è™•ç† AJAX è«‹æ±‚
                if node.get("data-request") == "web_listcomponent::onAttachmentDetail":
                    urls = self.web_parser.fetch_ajax_attachments(
                        node, self.base_url, page_url, document_title, self.doc_extensions
                    )
                else:
                    # è™•ç†ä¸€èˆ¬é€£çµ
                    href = self.web_parser.extract_href(node, self.base_url)
                    urls = [href] if href else []

                # å„²å­˜æ‰¾åˆ°çš„ URL
                for url in urls:
                    if url and url not in self.attachments_data[document_title]:
                        self.attachments_data[document_title].append(url)
                        self.pdf_links_set.add(url)
                        document_count += 1

                        logger.info(f"        âœ… {document_title}: {url}")

            return document_count

        except Exception as e:
            logger.error(f"çˆ¬å–é é¢å¤±æ•— {page_url}: {e}")
            return 0

    def _save_results(self) -> None:
        """å„²å­˜çˆ¬å–çµæœåˆ°æª”æ¡ˆ"""
        try:
            # å–å¾—å„²å­˜è·¯å¾‘
            data_dirs = self.config.get_data_directories()
            file_names = self.config.get_file_names()

            raw_data_dir = data_dirs["raw"]

            # å„²å­˜ PDF é€£çµåˆ—è¡¨
            pdf_links_file = f"{raw_data_dir}/{file_names['pdf_links']}"
            FileHandler.save_lines_to_file(
                pdf_links_file,
                list(self.pdf_links_set),
                sort_lines=True
            )

            # å„²å­˜è©³ç´°çš„é™„ä»¶è³‡æ–™
            personnel_data_file = f"{raw_data_dir}/{file_names['personnel_data']}"
            personnel_data = {
                "categories": dict(self.attachments_data),
                "total_documents": len(self.pdf_links_set),
                "crawl_timestamp": self._get_timestamp()
            }
            FileHandler.save_json_file(personnel_data_file, personnel_data)

            logger.info(f"å·²å„²å­˜ {len(self.pdf_links_set)} ç­† PDF é€£çµåˆ° {pdf_links_file}")
            logger.info(f"å·²å„²å­˜è©³ç´°è³‡æ–™åˆ° {personnel_data_file}")

        except Exception as e:
            logger.error(f"å„²å­˜çµæœå¤±æ•—: {e}")

    def _get_timestamp(self) -> str:
        """å–å¾—ç•¶å‰æ™‚é–“æˆ³è¨˜"""
        from datetime import datetime
        return datetime.now().isoformat()
